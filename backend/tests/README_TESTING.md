# Agent Testing Guide

## ğŸ¯ Overview

This directory contains test suites for all agents. Tests are designed to be **fast, predictable, and independent** by mocking external dependencies.

## ğŸ“‹ Test Structure

```
tests/
â”œâ”€â”€ conftest.py              # Shared fixtures and mocks
â”œâ”€â”€ test_bias_agent.py       # Bias Agent tests
â”œâ”€â”€ test_strategy_agent.py   # Strategy Agent tests
â”œâ”€â”€ test_risk_manager_agent.py  # Risk Manager tests
â””â”€â”€ README_TESTING.md        # This file
```

## ğŸ”§ Mocking Strategy

### **Problem: Unpredictable External Dependencies**

Without mocking, tests:
- âŒ Call real APIs (Finnhub, Data Plane)
- âŒ Depend on market conditions (tests fail randomly)
- âŒ Are SLOW (3+ minutes per test)
- âŒ Require all services running
- âŒ Cost money (API calls)

### **Solution: Mock All External Calls**

The `conftest.py` file provides automatic mocking:

```python
@pytest.fixture(autouse=True)
def auto_mock_tools(mock_all_tools):
    """Automatically mock tools for ALL tests."""
    # This runs for every test automatically
    pass
```

### **What Gets Mocked:**

1. **IndicatorTools** - Returns fake RSI, MACD, SMA values
2. **RSITool** - Returns predictable RSI analysis
3. **MACDTool** - Returns predictable MACD analysis
4. **Market Data** - Uses fixture data (no real API calls)

## ğŸ“Š Predictable Test Data

### **RSI Values (Mocked)**
```python
Timeframe â†’ RSI Value
"5m"  â†’ 52.3 (neutral, slightly bullish)
"15m" â†’ 48.7 (neutral, slightly bearish)
"1h"  â†’ 45.2 (neutral)
"4h"  â†’ 58.6 (neutral, approaching overbought)
"1d"  â†’ 42.8 (neutral)
```

### **MACD Values (Mocked)**
```python
Timeframe â†’ MACD / Signal / Histogram
"5m"  â†’ 0.8 / 0.6 / +0.2 (bullish)
"1h"  â†’ -0.3 / -0.1 / -0.2 (bearish)
"1d"  â†’ 1.2 / 0.9 / +0.3 (bullish)
```

### **Market Data (Fixture)**
```python
# 100 candles per timeframe
- Uptrend: $250 â†’ $260
- Realistic OHLC spread
- Volume: 1M+ per candle
- Timestamps: Sequential
```

## ğŸš€ Running Tests

### **Run All Bias Agent Tests**
```bash
cd backend
docker-compose exec backend pytest tests/test_bias_agent.py -v
```

### **Run with Detailed Output (Verbose)**
```bash
# Show test details: instructions, LLM output, expected vs actual
docker-compose exec backend pytest tests/test_bias_agent.py -v -s

# This will print for each test:
# - Input: Instructions, model, config
# - LLM Output: Full reasoning, bias, confidence
# - Expected vs Actual: Visual comparison
```

### **Run Single Test with Details**
```bash
docker-compose exec backend pytest tests/test_bias_agent.py::TestBiasAgentAccuracy::test_custom_rsi_thresholds_40_60 -v -s
```

### **Run Only Fast Tests (Exclude Slow)**
```bash
docker-compose exec backend pytest tests/test_bias_agent.py -m "not slow" -v
```

### **Run with Print Statements**
```bash
docker-compose exec backend pytest tests/test_bias_agent.py -v -s
```

### **Run Specific Category**
```bash
# Accuracy tests only
docker-compose exec backend pytest tests/ -m accuracy -v -s

# Unit tests only
docker-compose exec backend pytest tests/ -m unit -v

# Report tests only
docker-compose exec backend pytest tests/ -m report -v
```

## ğŸ·ï¸ Test Markers

Tests are categorized with markers:

- `@pytest.mark.accuracy` - Tests agent behavior accuracy
- `@pytest.mark.report` - Tests report generation
- `@pytest.mark.unit` - Fast unit tests
- `@pytest.mark.slow` - Long-running tests
- `@pytest.mark.integration` - Real API calls (not mocked)

## âœ… Test Checklist

When writing new tests:

1. âœ… Use `state_with_market_data` fixture for market data
2. âœ… Mocking is automatic (don't manually call APIs)
3. âœ… Use `model: "lm-studio"` for local testing
4. âœ… Add appropriate markers (`@pytest.mark.accuracy`, etc.)
5. âœ… Assert both success and failure cases
6. âœ… Check reasoning format (no artifacts)
7. âœ… Verify report generation
8. âœ… Test should complete in < 30 seconds

## ğŸ“ Example Test

```python
@pytest.mark.accuracy
def test_custom_rsi_thresholds_40_60(self, state_with_market_data):
    """Test: Agent should use custom RSI thresholds (40/60)."""
    registry = get_registry()
    
    # Configure agent with custom thresholds
    config = {
        "instructions": "Use RSI with 40/60 thresholds",
        "model": "lm-studio"  # Use local model
    }
    
    agent = registry.create_agent(
        agent_type="bias_agent",
        agent_id="test-bias",
        config=config
    )
    
    # Process with mocked data (fast!)
    result = agent.process(state_with_market_data)
    
    # Assert results
    assert result.biases["1d"].bias in ["BULLISH", "BEARISH", "NEUTRAL"]
    assert "40" in result.biases["1d"].reasoning  # Custom threshold used
```

## ğŸ” Debugging Failed Tests

### **Test Takes Too Long (>3 minutes)**
- âœ… Check if mocking is working
- âœ… Verify `auto_mock_tools` fixture is active
- âœ… Look for real API calls in logs

### **Test Results Vary**
- âœ… Ensure using `mock_all_tools` fixture
- âœ… Check if hardcoded test data is used
- âœ… Verify no environment-dependent logic

### **"Tool not found" Errors**
- âœ… Check tool registry is populated
- âœ… Verify tool names match exactly
- âœ… Ensure imports are correct

## ğŸ¯ Best Practices

### **DO:**
âœ… Mock external APIs  
âœ… Use fixtures for test data  
âœ… Test one thing per test  
âœ… Use descriptive test names  
âœ… Add docstrings explaining what's tested  
âœ… Assert specific values, not just "truthy"  

### **DON'T:**
âŒ Make real API calls in unit tests  
âŒ Test multiple behaviors in one test  
âŒ Use hardcoded IDs (use uuid4())  
âŒ Depend on test execution order  
âŒ Skip assertions ("it doesn't crash" isn't enough)  

## ğŸš¦ Test Coverage Goals

- **Bias Agent**: 80%+ coverage
- **Strategy Agent**: 80%+ coverage
- **Risk Manager**: 80%+ coverage
- **Trade Manager**: 70%+ coverage

## ğŸ“š Resources

- Pytest docs: https://docs.pytest.org/
- Monkeypatch: https://docs.pytest.org/en/stable/how-to/monkeypatch.html
- Fixtures: https://docs.pytest.org/en/stable/how-to/fixtures.html

## ğŸ› Troubleshooting

### Mocking Not Working?

1. Check `conftest.py` is in `tests/` directory
2. Verify `autouse=True` on `auto_mock_tools`
3. Restart pytest if fixtures were recently changed
4. Check import paths match exactly

### Tests Still Calling Real APIs?

1. Add print statement in mock to verify it's called
2. Check tool is instantiated AFTER mock is applied
3. Verify monkeypatch path is correct

## ğŸ“Š Current Status

- âœ… Mock fixtures created
- âœ… Auto-mocking enabled
- âœ… Predictable test data defined
- âœ… All tests use local model
- â³ Tests still slow (CrewAI overhead)
- â³ Some tests may still call Data Plane

**Next Steps:**
1. Verify mocking is fully working
2. Optimize test fixtures
3. Add more edge case tests
4. Improve test documentation

---

**Last Updated:** 2025-12-20  
**Maintained By:** Trading Platform Team

