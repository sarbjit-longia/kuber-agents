# Data Plane Service

**Version**: 1.0.0  
**Status**: Phase 1 Complete

## Overview

The Data Plane is a centralized market data service that provides cached, fast access to real-time and historical market data for all pipelines. It eliminates redundant API calls and dramatically reduces costs.

### Key Benefits

- âœ… **80-90% Cost Reduction**: One API call per ticker instead of one per pipeline execution
- âœ… **20-50x Faster**: <10ms from Redis cache vs 200-500ms from external API
- âœ… **No User API Keys**: Centralized Finnhub integration
- âœ… **Scalable**: 1,000 users = same API call volume
- âœ… **Free Market Data Agent**: No costs for users

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Data Plane Service                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  Universe Manager  â†’  Data Fetcher  â†’  Redis Cache               â”‚
â”‚  (Tracks tickers)     (Finnhub API)     (60s-5min TTL)           â”‚
â”‚                                                                   â”‚
â”‚  Hot Tickers (RUNNING): Fetch every 1 min                        â”‚
â”‚  Warm Tickers (ACTIVE): Fetch every 5 min                        â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â†“                       â†“
         Market Data Agent         Frontend Charts
         (reads from cache)        (future)
```

---

## Quick Start

### 1. Prerequisites

- Docker & Docker Compose
- Finnhub API key (free tier or professional)

### 2. Configuration

Add to your `.env` file:

```bash
FINNHUB_API_KEY=your_key_here
```

### 3. Start Services

```bash
# Start all services (including data-plane)
docker-compose up -d

# Check data-plane is running
docker ps | grep data-plane

# View logs
docker logs -f trading-data-plane
docker logs -f trading-data-plane-worker
docker logs -f trading-data-plane-beat
```

### 4. Verify

```bash
# Health check
curl http://localhost:8005/health

# Check universe
curl http://localhost:8005/api/v1/data/universe

# Get quote for AAPL
curl http://localhost:8005/api/v1/data/quote/AAPL

# Get candles
curl "http://localhost:8005/api/v1/data/candles/AAPL?timeframe=5m&limit=100"
```

---

## API Endpoints

### **GET `/api/v1/data/quote/{ticker}`**

Get latest quote (cached).

**Example**:
```bash
curl http://localhost:8005/api/v1/data/quote/AAPL
```

**Response**:
```json
{
  "ticker": "AAPL",
  "current_price": 178.45,
  "change": 2.15,
  "percent_change": 1.22,
  "high": 179.50,
  "low": 176.80,
  "open": 177.00,
  "previous_close": 176.30,
  "timestamp": "2025-12-10T19:45:00Z"
}
```

---

### **GET `/api/v1/data/candles/{ticker}`**

Get OHLCV candles.

**Query Parameters**:
- `timeframe`: 1m, 5m, 15m, 30m, 1h, 4h, 1d (default: 5m)
- `limit`: Number of candles (1-500, default: 100)

**Example**:
```bash
curl "http://localhost:8005/api/v1/data/candles/AAPL?timeframe=5m&limit=50"
```

**Response**:
```json
{
  "ticker": "AAPL",
  "timeframe": "5m",
  "count": 50,
  "candles": [
    {
      "ticker": "AAPL",
      "timeframe": "5m",
      "timestamp": "2025-12-10T19:40:00Z",
      "open": 178.20,
      "high": 178.50,
      "low": 178.10,
      "close": 178.45,
      "volume": 125000
    }
  ]
}
```

---

### **GET `/api/v1/data/batch`**

Batch fetch multiple tickers.

**Query Parameters**:
- `tickers`: Comma-separated list of tickers
- `data_types`: Comma-separated list (quote, candles)

**Example**:
```bash
curl "http://localhost:8005/api/v1/data/batch?tickers=AAPL,GOOGL,MSFT&data_types=quote"
```

---

### **GET `/api/v1/data/universe`**

Get current tracked tickers.

**Example**:
```bash
curl http://localhost:8005/api/v1/data/universe
```

**Response**:
```json
{
  "hot": ["AAPL", "GOOGL"],
  "warm": ["MSFT", "NVDA", "AMD"],
  "total": 5
}
```

---

## How It Works

### Universe Manager

Queries the backend database every 5 minutes to discover:
- **Hot Tickers**: From currently RUNNING executions
- **Warm Tickers**: From active pipelines/scanners

Stores in Redis sets: `tickers:hot` and `tickers:warm`.

### Data Fetcher

Celery Beat schedules:
- **Every 1 minute**: Fetch hot tickers (TTL = 60s)
- **Every 5 minutes**: Fetch warm tickers (TTL = 300s)

Caches quotes in Redis: `quote:{ticker}`

### Market Data Agent Integration

The Market Data Agent (Version 2.0) now calls the Data Plane API instead of Finnhub directly:

```python
# Old (Version 1.0): Direct Finnhub call per execution
quote = finnhub.quote("AAPL")  # 100 pipelines = 100 API calls

# New (Version 2.0): Data Plane cache
quote = data_plane_api.get_quote("AAPL")  # Cached! <10ms
```

---

## Monitoring

### Metrics Exposed

Data Plane exposes Prometheus metrics on port `8001`:

```bash
curl http://localhost:8006/metrics
```

**Key Metrics**:
- `universe_size{tier="hot"}` - Number of hot tickers
- `universe_size{tier="warm"}` - Number of warm tickers
- `quotes_fetched_total` - Total quotes fetched from Finnhub
- `quotes_cached_total{tier}` - Quotes cached by tier
- `quotes_fetch_failures_total` - Failed fetches
- `candles_fetched_total` - Candles fetched

### Grafana Dashboard

Add a new row in the Grafana dashboard:

**ðŸ“Š Data Plane Service**:
- Universe Size (Hot vs Warm)
- Quote Fetch Rate
- Cache Hit Rate
- Fetch Failures
- API Call Savings

---

## Configuration

### Environment Variables

```bash
# Service
SERVICE_NAME=data-plane
LOG_LEVEL=INFO

# Finnhub
FINNHUB_API_KEY=your_key_here

# Redis (separate database from main app)
REDIS_URL=redis://redis:6379/1

# TimescaleDB (for Phase 3: historical data storage)
TIMESCALE_URL=postgresql://dev:devpass@timescaledb:5432/trading_data_plane

# Backend DB (read-only, to query scanners/pipelines)
BACKEND_DB_URL=postgresql://dev:devpass@postgres:5432/trading_platform

# Celery
CELERY_BROKER_URL=redis://redis:6379/1
CELERY_RESULT_BACKEND=redis://redis:6379/1

# Metrics
METRICS_PORT=8001
```

---

## Troubleshooting

### No Data in Cache

**Check universe**:
```bash
# Should show tickers from active pipelines
curl http://localhost:8005/api/v1/data/universe
```

If empty, ensure you have:
1. Active pipelines with scanners
2. Data Plane services running
3. Universe refresh task running (check data-plane-beat logs)

### API Call Failures

**Check logs**:
```bash
docker logs trading-data-plane-worker --tail 50
```

Common issues:
- Invalid Finnhub API key
- Rate limit exceeded (upgrade to Pro tier)
- Network connectivity

### High Latency

**Check cache hit rate**:
```bash
# Should be >80% after 5 minutes
curl http://localhost:8006/metrics | grep quotes_cached
```

If low, check:
- Redis is running: `docker ps | grep redis`
- TTL settings (hot = 60s, warm = 300s)

---

## Roadmap

### Phase 1: Basic Caching (âœ… Complete)
- Universe discovery
- Quote caching (hot/warm tiers)
- On-demand candle fetching
- Market Data Agent integration

### Phase 2: Derived Indicators (Week 3)
- Pre-compute SMA (20, 50, 200)
- Pre-compute RSI (14), MACD
- Cache indicators in Redis
- `/data/indicators/{ticker}` endpoint

### Phase 3: TimescaleDB Storage (Week 4)
- Store historical OHLCV data
- Continuous aggregates (5m from 1m)
- Data compression
- Backfill historical data (500 days)

### Phase 4: Real-Time Streaming (Future)
- WebSocket endpoint `/data/stream/{ticker}`
- Sub-second price updates
- Frontend real-time charts (TradingView)

### Phase 5: Multi-Provider (Future)
- Fallback providers (Alpha Vantage, Polygon)
- Provider health monitoring
- Cost optimization (cheapest provider per ticker)

---

## Development

### Project Structure

```
data-plane/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ config.py            # Settings
â”‚   â”œâ”€â”€ database.py          # DB connections
â”‚   â”œâ”€â”€ telemetry.py         # OpenTelemetry
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ ohlcv.py         # TimescaleDB model
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ universe_manager.py
â”‚   â”‚   â””â”€â”€ data_fetcher.py
â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â””â”€â”€ celery_tasks.py  # Scheduled tasks
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ v1/
â”‚           â””â”€â”€ data.py      # API endpoints
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

### Running Locally

```bash
# Install dependencies
cd data-plane
pip install -r requirements.txt

# Set environment variables
export FINNHUB_API_KEY=your_key
export REDIS_URL=redis://localhost:6379/1
# ... other vars

# Run API
uvicorn app.main:app --reload --port 8000

# Run worker
celery -A app.tasks.celery_tasks worker --loglevel=INFO

# Run beat
celery -A app.tasks.celery_tasks beat --loglevel=INFO
```

### Adding New Endpoints

1. Add endpoint in `app/api/v1/data.py`
2. Add metrics in service layer
3. Update this README
4. Test with curl/httpx

---

## Performance

### Benchmarks (Local Docker)

| Operation | Before (Direct API) | After (Cache) | Improvement |
|-----------|---------------------|---------------|-------------|
| Quote Fetch | 250ms | 8ms | **31x faster** |
| 100 Candles | 450ms | 450ms* | Same (on-demand)** |
| 5 Tickers Batch | 1,250ms | 40ms | **31x faster** |

\* Phase 3 will cache candles in TimescaleDB  
\*\* Phase 1 fetches on-demand, Phase 3 will cache

### Cost Savings (100 Users Example)

| Item | Before | After | Savings |
|------|--------|-------|---------|
| API Calls/Day | 4,000 | 500 | 87.5% |
| Finnhub Cost | $49/month | $49/month* | â€” |
| Latency (avg) | 250ms | 10ms | 96% |

\* Same tier handles 10x more users

---

## Support

- **Documentation**: `/docs/services/signal-system.md` (includes Data Plane)
- **Issues**: GitHub Issues
- **Logs**: `docker logs trading-data-plane`

---

**Related Services**:
- [Signal Generator](../signal-generator/README.md)
- [Trigger Dispatcher](../trigger-dispatcher/README.md)
- [Monitoring Setup](../monitoring/README.md)

