# Signal System Architecture & Guide

**Last Updated**: December 2025  
**Status**: Production Ready

## Overview

The Signal System is an event-driven architecture that enables pipelines to execute in response to market conditions. It consists of three main components:

1. **Signal Generators** - Monitor markets and emit signals when conditions are met
2. **Kafka** - Message bus for signal distribution
3. **Trigger Dispatcher** - Matches signals to pipelines and triggers execution

```
┌──────────────────────────────────────────────────────────────────┐
│                      Signal System Flow                          │
└──────────────────────────────────────────────────────────────────┘

  1. SIGNAL GENERATION                2. DISTRIBUTION              3. MATCHING & EXECUTION
  
┌────────────────────┐            ┌──────────────┐           ┌──────────────────────┐
│ Signal Generators  │   publish  │    Kafka     │ subscribe │ Trigger Dispatcher   │
│                    │──────────>│              │─────────>│                      │
│ • Mock Signal      │            │ Topic:       │           │ • Cache Pipelines    │
│ • Golden Cross     │            │ trading-     │           │ • Match Signals      │
│ • News Sentiment   │            │ signals      │           │ • Enqueue Tasks      │
│ • Custom...        │            │              │           │                      │
└────────────────────┘            └──────────────┘           └──────────┬───────────┘
                                                                        │
                                                                        ▼
                                                              ┌──────────────────────┐
                                                              │  Celery Workers      │
                                                              │  (Execute Pipelines) │
                                                              └──────────────────────┘
```

---

## 1. Signal Generators

### 1.1 Purpose

Signal Generators continuously monitor market data and technical indicators, emitting structured signals to Kafka when conditions are met. Each generator is independent and can run in its own container.

### 1.2 Available Generators

#### **Mock Generator** (Testing)
- **Purpose**: Generates random test signals for development
- **Frequency**: Configurable (default: every 60s)
- **Tickers**: From watchlist.json
- **Signals**: Random BULLISH/BEARISH/NEUTRAL with confidence 70-95%

#### **Golden Cross Generator**
- **Purpose**: Detects SMA 50/200 crossovers
- **Frequency**: Every 5 minutes
- **Tickers**: From watchlist.json
- **Signals**: BULLISH (50 crosses above 200), confidence based on volume

#### **Future Generators**
- News Sentiment (AI-powered)
- RSI Overbought/Oversold
- Volatility Breakout
- Support/Resistance Touch
- Custom User-Defined

### 1.3 Signal Schema

All signals must conform to this standardized schema:

```json
{
  "timestamp": 1702234567,
  "source": "golden_cross",
  "signal_id": "gc_1702234567_AAPL",
  "signal_type": "golden_cross",
  "tickers": [
    {
      "ticker": "AAPL",
      "signal": "BULLISH",
      "confidence": 85.5
    }
  ]
}
```

**Field Definitions**:
- `timestamp`: Unix epoch (seconds)
- `source`: Identifier for the generator
- `signal_id`: Unique ID for deduplication
- `signal_type`: Type of signal (matches subscription types)
- `tickers`: Array of ticker-specific signals
  - `ticker`: Stock symbol
  - `signal`: BULLISH, BEARISH, or NEUTRAL
  - `confidence`: 0-100 confidence score

### 1.4 Configuration

**File**: `signal-generator/config/watchlist.json`

```json
{
  "watchlist": [
    "AAPL", "GOOGL", "MSFT", "TSLA", "NVDA",
    "AMD", "META", "AMZN", "NFLX", "SPY"
  ]
}
```

**Environment Variables**:
```bash
# Service
SERVICE_NAME=signal-generator
LOG_LEVEL=INFO

# Kafka
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_SIGNAL_TOPIC=trading-signals

# Finnhub API (for real market data)
FINNHUB_API_KEY=your_key_here

# Signal Generation
SIGNAL_INTERVAL=60  # seconds between signal generations
```

### 1.5 Running Signal Generator

```bash
# Via Docker Compose
docker-compose up -d signal-generator

# View logs
docker logs -f signal-generator

# Metrics endpoint
curl http://localhost:8003/metrics
```

### 1.6 Metrics

Signal Generator exposes Prometheus metrics:

- `signals_generated_total` - Total signals generated by type
- `kafka_publish_success_total` - Successful Kafka publishes
- `kafka_publish_failure_total` - Failed Kafka publishes
- `kafka_publish_duration_seconds` - Publish latency
- `signal_generation_duration_seconds` - Time to generate signals

---

## 2. Kafka Integration

### 2.1 Architecture

**Zookeeper + Kafka** containers provide the message bus.

**Topic**: `trading-signals`
- **Partitions**: 3 (for parallelism)
- **Retention**: 24 hours
- **Compression**: snappy

### 2.2 Configuration

**File**: `docker-compose.yml`

```yaml
zookeeper:
  image: confluentinc/cp-zookeeper:latest
  environment:
    ZOOKEEPER_CLIENT_PORT: 2181

kafka:
  image: confluentinc/cp-kafka:latest
  environment:
    KAFKA_BROKER_ID: 1
    KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
  ports:
    - "9092:9092"
```

### 2.3 Testing Kafka

```bash
# Check Kafka is running
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092

# Create topic manually (if needed)
docker exec kafka kafka-topics --create \
  --topic trading-signals \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1

# Consume messages (for testing)
docker exec kafka kafka-console-consumer \
  --topic trading-signals \
  --from-beginning \
  --bootstrap-server localhost:9092
```

---

## 3. Trigger Dispatcher

### 3.1 Purpose

The Trigger Dispatcher is a lightweight service that:
1. Subscribes to the Kafka `trading-signals` topic
2. Maintains an in-memory cache of active signal-based pipelines
3. Matches incoming signals to pipelines based on:
   - Scanner tickers
   - Signal subscriptions
   - Confidence thresholds
4. Enqueues matched pipelines for execution (via Celery)

### 3.2 Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                  Trigger Dispatcher                          │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌────────────────────────────────────────────┐            │
│  │  Pipeline Cache (In-Memory)                │            │
│  │  {                                          │            │
│  │    "pipeline_uuid": {                      │            │
│  │      "user_id": "...",                     │            │
│  │      "scanner_tickers": ["AAPL", "GOOGL"],│            │
│  │      "signal_subscriptions": [...]         │            │
│  │    }                                        │            │
│  │  }                                          │            │
│  └────────────────────────────────────────────┘            │
│                                                              │
│  ┌────────────────────────────────────────────┐            │
│  │  Signal Processor                          │            │
│  │  • Batch consume from Kafka                │            │
│  │  • Match signals to pipelines              │            │
│  │  • Check if pipeline already running       │            │
│  │  • Enqueue to Celery                       │            │
│  └────────────────────────────────────────────┘            │
│                                                              │
└─────────────────────────────────────────────────────────────┘
         │                                        │
         ▼                                        ▼
    PostgreSQL                              Celery/Redis
  (Read Pipelines)                       (Enqueue Tasks)
```

### 3.3 Matching Logic

For each incoming signal:
1. **Extract tickers** from signal
2. **Find pipelines** with matching tickers in their scanner
3. **Check signal subscriptions**:
   - Does pipeline subscribe to this signal type?
   - Does confidence meet minimum threshold?
4. **Check if already running**:
   - Query executions table for PENDING/RUNNING status
   - Skip if already executing
5. **Enqueue** via Celery task

### 3.4 Configuration

**Environment Variables**:
```bash
# Service
SERVICE_NAME=trigger-dispatcher
LOG_LEVEL=INFO

# Database (read-only for pipeline cache)
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_USER=dev
POSTGRES_PASSWORD=devpass
POSTGRES_DB=trading_platform

# Kafka
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_SIGNAL_TOPIC=trading-signals
KAFKA_CONSUMER_GROUP=trigger-dispatcher
KAFKA_AUTO_OFFSET_RESET=latest

# Celery (for enqueuing tasks)
CELERY_BROKER_URL=redis://redis:6379/0

# Performance
CACHE_REFRESH_INTERVAL=300  # seconds (5 min)
BATCH_SIZE=100  # signals per batch
MAX_CONCURRENT_TASKS=10
```

### 3.5 Running Trigger Dispatcher

```bash
# Via Docker Compose
docker-compose up -d trigger-dispatcher

# View logs
docker logs -f trigger-dispatcher

# Metrics endpoint
curl http://localhost:8004/metrics
```

### 3.6 Metrics

Trigger Dispatcher exposes:

- `pipeline_cache_size` - Number of cached pipelines
- `signals_consumed_total` - Signals consumed from Kafka
- `pipelines_matched_total` - Pipelines matched to signals
- `pipelines_enqueued_total` - Pipelines enqueued for execution
- `pipelines_skipped_running_total` - Pipelines skipped (already running)
- `batch_processing_duration_seconds` - Time to process signal batch

---

## 4. Pipeline Configuration

### 4.1 Trigger Modes

Pipelines have a `trigger_mode` field:

- **`signal`**: Triggered by matching signals from Kafka
- **`periodic`**: Runs on a schedule (Celery Beat)

### 4.2 Signal-Based Pipeline Setup

1. **Create a Scanner** with tickers (e.g., "Tech Stocks")
2. **Create a Pipeline** and link to scanner
3. **Set trigger_mode** = `signal`
4. **Add Signal Subscriptions**:
   ```json
   {
     "signal_subscriptions": [
       {
         "signal_type": "golden_cross",
         "min_confidence": 80
       },
       {
         "signal_type": "news_sentiment",
         "min_confidence": 70
       }
     ]
   }
   ```
5. **Activate Pipeline** - Trigger Dispatcher will cache it

### 4.3 Scanner Configuration

```json
{
  "id": "uuid",
  "name": "Tech Stocks",
  "scanner_type": "manual",
  "tickers": ["AAPL", "GOOGL", "MSFT", "NVDA", "AMD"]
}
```

---

## 5. Monitoring & Observability

All three services (Signal Generator, Trigger Dispatcher, Backend) expose Prometheus metrics.

### 5.1 Metrics Ports

- Backend: `8001`
- Celery Worker: `8002`
- Signal Generator: `8003`
- Trigger Dispatcher: `8004`

### 5.2 Grafana Dashboard

View in Grafana at `http://localhost:3000`:

- **Signal Generator Service** row shows signal generation rate, Kafka publish metrics
- **Trigger Dispatcher Service** row shows cached pipelines, matching rate, enqueue rate
- **System Health** row shows active pipelines, success rate, etc.

### 5.3 Key Metrics to Monitor

**Signal Generator**:
- Signals generated per minute
- Kafka publish failures
- Publish latency

**Trigger Dispatcher**:
- Cached pipelines count
- Signals consumed rate
- Pipelines matched vs skipped
- Batch processing latency

**Overall System**:
- End-to-end latency (signal → execution start)
- Signal-to-trade conversion rate
- Failed pipeline executions

---

## 6. Troubleshooting

### 6.1 Signals Not Being Generated

```bash
# Check signal generator logs
docker logs signal-generator --tail 100

# Check Kafka is running
docker ps | grep kafka

# Verify watchlist.json
cat signal-generator/config/watchlist.json

# Check metrics
curl http://localhost:8003/metrics | grep signals_generated
```

### 6.2 Pipelines Not Triggering

```bash
# Check trigger dispatcher logs
docker logs trigger-dispatcher --tail 100

# Verify pipeline is cached
curl http://localhost:8004/metrics | grep pipeline_cache_size

# Check if pipeline is active and signal-based
docker exec trading-backend psql -U dev -d trading_platform \
  -c "SELECT id, name, is_active, trigger_mode FROM pipelines;"

# Check signal subscriptions
docker exec trading-backend psql -U dev -d trading_platform \
  -c "SELECT name, signal_subscriptions FROM pipelines WHERE trigger_mode='signal';"
```

### 6.3 Kafka Connection Issues

```bash
# Check Kafka health
docker exec kafka kafka-broker-api-versions --bootstrap-server localhost:9092

# Check topic exists
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092

# Check consumer group lag
docker exec kafka kafka-consumer-groups --bootstrap-server localhost:9092 \
  --group trigger-dispatcher --describe
```

### 6.4 High Latency

**Symptoms**: Signals generated but executions delayed

**Check**:
1. Celery queue size: `docker exec trading-backend redis-cli llen celery`
2. Celery worker concurrency: Increase in `docker-compose.yml`
3. Trigger dispatcher batch size: Tune `BATCH_SIZE` env var
4. Database connection pool: Check PostgreSQL logs

---

## 7. Performance Tuning

### 7.1 Signal Generator

**Reduce API calls**:
```bash
# Increase signal interval
SIGNAL_INTERVAL=300  # 5 minutes instead of 60 seconds
```

**Reduce ticker count**:
- Edit `watchlist.json` to include only actively traded tickers

### 7.2 Trigger Dispatcher

**Increase throughput**:
```bash
BATCH_SIZE=500  # Process more signals per batch
MAX_CONCURRENT_TASKS=20  # Enqueue more pipelines in parallel
```

**Reduce cache refresh**:
```bash
CACHE_REFRESH_INTERVAL=600  # 10 minutes (only if pipelines change infrequently)
```

### 7.3 Kafka

**Increase partitions** for parallelism:
```bash
docker exec kafka kafka-topics --alter \
  --topic trading-signals \
  --partitions 6 \
  --bootstrap-server localhost:9092
```

**Tune retention** (reduce storage):
```bash
# In docker-compose.yml, add to kafka environment:
KAFKA_LOG_RETENTION_HOURS=12  # 12 hours instead of 24
```

---

## 8. Development Guide

### 8.1 Adding a New Signal Generator

1. **Create generator class** in `signal-generator/app/generators/`
2. **Implement required methods**:
   ```python
   class MyGenerator(BaseGenerator):
       def generate_signals(self) -> List[Signal]:
           # Your logic here
           pass
   ```
3. **Register in main.py**:
   ```python
   generators = [
       MockGenerator(config),
       GoldenCrossGenerator(config),
       MyGenerator(config)  # Add here
   ]
   ```
4. **Update subscription options** in backend signal buckets

### 8.2 Testing Signal Flow

```bash
# 1. Start all services
docker-compose up -d

# 2. Create a scanner with test tickers
# Use frontend or API: POST /api/v1/scanners

# 3. Create signal-based pipeline
# Use Pipeline Builder, set trigger_mode=signal

# 4. Monitor signal generation
docker logs -f signal-generator

# 5. Monitor trigger dispatcher
docker logs -f trigger-dispatcher

# 6. Check execution triggered
docker logs -f trading-celery-worker | grep "pipeline_execution"
```

### 8.3 Local Development Without Kafka

For testing, you can bypass Kafka and trigger pipelines manually:

```bash
# Direct execution via API
curl -X POST http://localhost:8000/api/v1/executions \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"pipeline_id": "uuid", "mode": "paper"}'
```

---

## 9. Production Considerations

### 9.1 Scaling

**Horizontal Scaling**:
- Run multiple Signal Generator instances (different generators per instance)
- Run multiple Trigger Dispatcher instances (Kafka consumer group auto-balances)
- Run multiple Celery workers

**Kafka Scaling**:
- Increase partitions for parallel processing
- Add Kafka brokers for high availability

### 9.2 High Availability

**Kafka**:
- Run 3+ Kafka brokers with replication factor 2+
- Run 3+ Zookeeper nodes

**Services**:
- Run 2+ instances of each service behind load balancer
- Health checks on all containers

### 9.3 Security

**Kafka**:
- Enable SSL/TLS for encryption
- Enable SASL for authentication
- Network isolation (VPC/subnets)

**Signal Generators**:
- Encrypt API keys (AWS Secrets Manager)
- Rate limit external API calls
- Validate all signals before publishing

---

## 10. Future Enhancements

- [ ] Signal replay/backtesting
- [ ] Signal quality scoring
- [ ] A/B testing for signals
- [ ] User-defined custom signals (upload Python code)
- [ ] Signal marketplace (buy/sell signal feeds)
- [ ] Multi-exchange support (crypto, forex)
- [ ] Real-time signal dashboard
- [ ] Signal correlation analysis

---

**Related Documentation**:
- [Monitoring System](./monitoring.md)
- [Scanner System](./scanner.md)
- [Subscription & Billing](../roadmap.md#subscription--billing-roadmap)

