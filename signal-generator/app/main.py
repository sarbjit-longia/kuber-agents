"""
Signal Generator Service

Main entry point for the signal generator service.
Runs configured signal generators and emits signals to stdout (Phase 1)
or Kafka (Phase 2).
"""
import asyncio
import json
import time
from collections import deque
from copy import deepcopy
from datetime import datetime
from typing import List, Optional
import structlog
from kafka import KafkaProducer
from kafka.errors import KafkaError

from app.config import settings
from app.generators import (
    get_registry,
    # MockSignalGenerator removed - not for production use
    GoldenCrossSignalGenerator,
    DeathCrossSignalGenerator,
    RSISignalGenerator,
    MACDSignalGenerator,
    VolumeSpikeSignalGenerator,
    BollingerBandsSignalGenerator,
    StochasticSignalGenerator,
    ADXSignalGenerator,
    EMACrossoverSignalGenerator,
    ATRSignalGenerator,
    CCISignalGenerator,
    StochRSISignalGenerator,
    WilliamsRSignalGenerator,
    AroonSignalGenerator,
    MFISignalGenerator,
    OBVSignalGenerator,
    SARSignalGenerator,
    EMA200CrossoverSignalGenerator,
    SwingPointBreakSignalGenerator,
    MomentumDivergenceSignalGenerator,
    FairValueGapSignalGenerator,
    LiquiditySweepSignalGenerator,
    BreakOfStructureSignalGenerator,
    OrderBlockSignalGenerator,
    ChangeOfCharacterSignalGenerator,
    VolumeProfilePOCSignalGenerator,
    AccumulationDistributionSignalGenerator,
    HTFTrendAlignmentSignalGenerator
)
from app.schemas.signal import Signal
from app.telemetry import setup_telemetry


# Configure structured logging
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.add_log_level,
        structlog.processors.JSONRenderer()
    ]
)

logger = structlog.get_logger()


class SignalGeneratorService:
    """
    Signal generator service that runs multiple generators.
    
    In Phase 1, signals are emitted to stdout/logs.
    In Phase 2, signals are published to Kafka.
    """
    
    def __init__(self):
        """Initialize the service with configured generators."""
        self.registry = get_registry()
        self.generators = []
        self.running = False
        self.kafka_producer: Optional[KafkaProducer] = None
        self.scanner_universe = None
        
        # Recent signals buffer (keep last 50 signals)
        self.recent_signals = deque(maxlen=50)
        
        # Initialize Scanner Universe Manager if DB URL provided
        if settings.BACKEND_DB_URL:
            try:
                from app.scanner_universe import ScannerUniverseManager
                self.scanner_universe = ScannerUniverseManager(settings.BACKEND_DB_URL)
                self.scanner_universe.connect()
                logger.info("scanner_universe_manager_initialized")
            except Exception as e:
                logger.error("scanner_universe_manager_initialization_failed", error=str(e))
                self.scanner_universe = None
        else:
            logger.warning("backend_db_url_not_configured", 
                          message="Will use static watchlist instead of dynamic scanner universe")
        
        # Initialize telemetry
        try:
            self.meter = setup_telemetry(service_name="signal-generator")
            self._setup_metrics()
            logger.info("telemetry_initialized")
        except Exception as e:
            logger.error("telemetry_initialization_failed", error=str(e))
            self.meter = None
        
        # Initialize Kafka producer
        self._initialize_kafka()
    
    def _setup_metrics(self):
        """Setup custom metrics."""
        if not self.meter:
            return
        
        # Signal generation metrics
        self.signals_generated = self.meter.create_counter(
            "signals_generated_total",
            description="Total signals generated by generator and signal type"
        )
        
        self.signal_generation_duration = self.meter.create_histogram(
            "signal_generation_duration_seconds",
            description="Time taken to generate signals by generator"
        )
        
        # Generator scan metrics
        self.generator_scans_total = self.meter.create_counter(
            "generator_scans_total",
            description="Total number of scans per generator"
        )
        
        self.generator_scan_errors = self.meter.create_counter(
            "generator_scan_errors_total",
            description="Total scan errors per generator"
        )
        
        # Kafka metrics
        self.kafka_publish_duration = self.meter.create_histogram(
            "kafka_publish_duration_seconds",
            description="Time taken to publish signal to Kafka"
        )
        
        self.kafka_publish_success = self.meter.create_counter(
            "kafka_publish_success_total",
            description="Total successful Kafka publishes by signal type"
        )
        
        self.kafka_publish_failure = self.meter.create_counter(
            "kafka_publish_failure_total",
            description="Total failed Kafka publishes by signal type"
        )
        
        # Finnhub API metrics
        self.finnhub_api_calls = self.meter.create_counter(
            "finnhub_api_calls_total",
            description="Total Finnhub API calls by indicator type"
        )
        
        self.finnhub_api_errors = self.meter.create_counter(
            "finnhub_api_errors_total",
            description="Total Finnhub API errors by error type"
        )
        
        # Initialize generators based on configuration
        self._initialize_generators()

    def _merge_timeframes(self, primary: str) -> List[str]:
        """Combine primary timeframe with additional timeframes (dedup, preserve order)."""
        tfs = []
        if primary:
            tfs.append(primary)

        # Parse comma-separated string into list
        if settings.ADDITIONAL_TIMEFRAMES:
            extra = [tf.strip() for tf in settings.ADDITIONAL_TIMEFRAMES.split(",") if tf.strip()]
            for tf in extra:
                if tf and tf not in tfs:
                    tfs.append(tf)
        return tfs or ["D"]
    
    def _initialize_kafka(self):
        """Initialize Kafka producer for signal publishing."""
        try:
            self.kafka_producer = KafkaProducer(
                bootstrap_servers=settings.KAFKA_BOOTSTRAP_SERVERS,
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                key_serializer=lambda k: k.encode('utf-8') if k else None,
                acks='all',  # Wait for all replicas to acknowledge
                retries=3,
                max_in_flight_requests_per_connection=1  # Ensure ordering
            )
            logger.info(
                "kafka_producer_initialized",
                bootstrap_servers=settings.KAFKA_BOOTSTRAP_SERVERS,
                topic=settings.KAFKA_SIGNAL_TOPIC
            )
        except Exception as e:
            logger.error(
                "kafka_producer_initialization_failed",
                error=str(e),
                exc_info=True
            )
            self.kafka_producer = None
            logger.warning("signals_will_only_log", message="Kafka unavailable, falling back to logs only")
    
    def _initialize_generators(self):
        """Initialize all configured generators."""
        # Get watchlist from Scanner Universe Manager or fallback to static config
        if self.scanner_universe:
            try:
                watchlist = self.scanner_universe.get_active_scanner_tickers()
                if not watchlist:
                    logger.warning("no_active_scanner_tickers", 
                                  message="No tickers found from active scanners, falling back to static watchlist")
                    watchlist = settings.get_watchlist()
            except Exception as e:
                logger.error("failed_to_get_scanner_tickers", error=str(e))
                watchlist = settings.get_watchlist()
        else:
            watchlist = settings.get_watchlist()
        
        logger.info("initializing_generators_with_watchlist", ticker_count=len(watchlist), tickers=watchlist)
        
        # NOTE: Mock generator removed - not for production use
        # from app.schemas.signal import BiasType
        # mock_config = {"tickers": watchlist[:3], "emission_probability": 0.3, "bias_options": [BiasType.BULLISH, BiasType.BEARISH]}
        # self.generators.append({"name": "mock", "generator": MockSignalGenerator(mock_config), "interval": settings.MOCK_GENERATOR_INTERVAL_SECONDS})
        
        # Golden cross generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.GOLDEN_CROSS_TIMEFRAME):
            golden_cross_config = {
                "tickers": watchlist,
                "sma_short": settings.GOLDEN_CROSS_SMA_SHORT,
                "sma_long": settings.GOLDEN_CROSS_SMA_LONG,
                "timeframe": tf,
                "lookback_days": 5,
                "confidence": 0.85
            }
            self.generators.append({
                "name": f"golden_cross_{tf}",
                "generator": GoldenCrossSignalGenerator(golden_cross_config),
                "interval": settings.GOLDEN_CROSS_CHECK_INTERVAL_SECONDS
            })
        
        # Death cross generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.DEATH_CROSS_TIMEFRAME):
            death_cross_config = {
                "tickers": watchlist,
                "sma_short": settings.DEATH_CROSS_SMA_SHORT,
                "sma_long": settings.DEATH_CROSS_SMA_LONG,
                "timeframe": tf,
                "lookback_days": 5,
                "confidence": 0.85
            }
            self.generators.append({
                "name": f"death_cross_{tf}",
                "generator": DeathCrossSignalGenerator(death_cross_config),
                "interval": settings.DEATH_CROSS_CHECK_INTERVAL_SECONDS
            })
        
        # RSI generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.RSI_TIMEFRAME):
            rsi_config = {
                "tickers": watchlist,
                "period": settings.RSI_PERIOD,
                "oversold_threshold": settings.RSI_OVERSOLD_THRESHOLD,
                "overbought_threshold": settings.RSI_OVERBOUGHT_THRESHOLD,
                "timeframe": tf,
                "confidence": 0.75
            }
            self.generators.append({
                "name": f"rsi_{tf}",
                "generator": RSISignalGenerator(rsi_config),
                "interval": settings.RSI_CHECK_INTERVAL_SECONDS
            })
        
        # MACD generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.MACD_TIMEFRAME):
            macd_config = {
                "tickers": watchlist,
                "fast_period": settings.MACD_FAST_PERIOD,
                "slow_period": settings.MACD_SLOW_PERIOD,
                "signal_period": settings.MACD_SIGNAL_PERIOD,
                "timeframe": tf,
                "confidence": 0.80,
                "require_histogram_confirmation": True
            }
            self.generators.append({
                "name": f"macd_{tf}",
                "generator": MACDSignalGenerator(macd_config),
                "interval": settings.MACD_CHECK_INTERVAL_SECONDS
            })
        
        # Volume Spike generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.VOLUME_SPIKE_TIMEFRAME):
            volume_spike_config = {
                "tickers": watchlist,
                "volume_period": settings.VOLUME_SPIKE_PERIOD,
                "spike_threshold": settings.VOLUME_SPIKE_THRESHOLD,
                "timeframe": tf,
                "confidence": 0.70,
                "use_price_direction": True,
                "min_price_change_pct": 1.0
            }
            self.generators.append({
                "name": f"volume_spike_{tf}",
                "generator": VolumeSpikeSignalGenerator(volume_spike_config),
                "interval": settings.VOLUME_SPIKE_CHECK_INTERVAL_SECONDS
            })
        
        # Bollinger Bands generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.BBANDS_TIMEFRAME):
            bbands_config = {
                "tickers": watchlist,
                "timeperiod": settings.BBANDS_TIMEPERIOD,
                "nbdevup": settings.BBANDS_NBDEVUP,
                "nbdevdn": settings.BBANDS_NBDEVDN,
                "timeframe": tf,
                "confidence": 0.75,
                "signal_type": settings.BBANDS_SIGNAL_TYPE
            }
            self.generators.append({
                "name": f"bollinger_bands_{tf}",
                "generator": BollingerBandsSignalGenerator(bbands_config),
                "interval": settings.BBANDS_CHECK_INTERVAL_SECONDS
            })
        
        # Stochastic generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.STOCH_TIMEFRAME):
            stoch_config = {
                "tickers": watchlist,
                "fastk_period": settings.STOCH_FASTK_PERIOD,
                "slowk_period": settings.STOCH_SLOWK_PERIOD,
                "slowd_period": settings.STOCH_SLOWD_PERIOD,
                "overbought": settings.STOCH_OVERBOUGHT,
                "oversold": settings.STOCH_OVERSOLD,
                "timeframe": tf,
                "confidence": 0.75
            }
            self.generators.append({
                "name": f"stochastic_{tf}",
                "generator": StochasticSignalGenerator(stoch_config),
                "interval": settings.STOCH_CHECK_INTERVAL_SECONDS
            })
        
        # ADX generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.ADX_TIMEFRAME):
            adx_config = {
                "tickers": watchlist,
                "timeperiod": settings.ADX_TIMEPERIOD,
                "strong_trend": settings.ADX_STRONG_TREND,
                "weak_trend": settings.ADX_WEAK_TREND,
                "timeframe": tf,
                "confidence": 0.70
            }
            self.generators.append({
                "name": f"adx_{tf}",
                "generator": ADXSignalGenerator(adx_config),
                "interval": settings.ADX_CHECK_INTERVAL_SECONDS
            })
        
        # EMA Crossover generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.EMA_TIMEFRAME):
            ema_config = {
                "tickers": watchlist,
                "ema_fast": settings.EMA_FAST,
                "ema_slow": settings.EMA_SLOW,
                "timeframe": tf,
                "lookback_days": 5,
                "confidence": 0.80
            }
            self.generators.append({
                "name": f"ema_crossover_{tf}",
                "generator": EMACrossoverSignalGenerator(ema_config),
                "interval": settings.EMA_CHECK_INTERVAL_SECONDS
            })
        
        # ATR generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.ATR_TIMEFRAME):
            atr_config = {
                "tickers": watchlist,
                "timeperiod": settings.ATR_TIMEPERIOD,
                "spike_multiplier": settings.ATR_SPIKE_MULTIPLIER,
                "compression_multiplier": settings.ATR_COMPRESSION_MULTIPLIER,
                "lookback_for_average": settings.ATR_LOOKBACK_FOR_AVERAGE,
                "timeframe": tf,
                "confidence": 0.65
            }
            self.generators.append({
                "name": f"atr_{tf}",
                "generator": ATRSignalGenerator(atr_config),
                "interval": settings.ATR_CHECK_INTERVAL_SECONDS
            })
        
        # CCI generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.CCI_TIMEFRAME):
            cci_config = {
                "tickers": watchlist,
                "timeperiod": settings.CCI_TIMEPERIOD,
                "overbought": settings.CCI_OVERBOUGHT,
                "oversold": settings.CCI_OVERSOLD,
                "timeframe": tf,
                "confidence": 0.70
            }
            self.generators.append({
                "name": f"cci_{tf}",
                "generator": CCISignalGenerator(cci_config),
                "interval": settings.CCI_CHECK_INTERVAL_SECONDS
            })
        
        # Stochastic RSI generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.STOCHRSI_TIMEFRAME):
            stochrsi_config = {
                "tickers": watchlist,
                "timeperiod": settings.STOCHRSI_TIMEPERIOD,
                "fastk_period": settings.STOCHRSI_FASTK_PERIOD,
                "fastd_period": settings.STOCHRSI_FASTD_PERIOD,
                "overbought": settings.STOCHRSI_OVERBOUGHT,
                "oversold": settings.STOCHRSI_OVERSOLD,
                "timeframe": tf,
                "confidence": 0.75
            }
            self.generators.append({
                "name": f"stochrsi_{tf}",
                "generator": StochRSISignalGenerator(stochrsi_config),
                "interval": settings.STOCHRSI_CHECK_INTERVAL_SECONDS
            })
        
        # Williams %R generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.WILLR_TIMEFRAME):
            willr_config = {
                "tickers": watchlist,
                "timeperiod": settings.WILLR_TIMEPERIOD,
                "overbought": settings.WILLR_OVERBOUGHT,
                "oversold": settings.WILLR_OVERSOLD,
                "timeframe": tf,
                "confidence": 0.70
            }
            self.generators.append({
                "name": f"williams_r_{tf}",
                "generator": WilliamsRSignalGenerator(willr_config),
                "interval": settings.WILLR_CHECK_INTERVAL_SECONDS
            })
        
        # AROON generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.AROON_TIMEFRAME):
            aroon_config = {
                "tickers": watchlist,
                "timeperiod": settings.AROON_TIMEPERIOD,
                "trend_threshold": settings.AROON_TREND_THRESHOLD,
                "timeframe": tf,
                "confidence": 0.75
            }
            self.generators.append({
                "name": f"aroon_{tf}",
                "generator": AroonSignalGenerator(aroon_config),
                "interval": settings.AROON_CHECK_INTERVAL_SECONDS
            })
        
        # MFI generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.MFI_TIMEFRAME):
            mfi_config = {
                "tickers": watchlist,
                "timeperiod": settings.MFI_TIMEPERIOD,
                "overbought": settings.MFI_OVERBOUGHT,
                "oversold": settings.MFI_OVERSOLD,
                "timeframe": tf,
                "confidence": 0.75
            }
            self.generators.append({
                "name": f"mfi_{tf}",
                "generator": MFISignalGenerator(mfi_config),
                "interval": settings.MFI_CHECK_INTERVAL_SECONDS
            })
        
        # OBV generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.OBV_TIMEFRAME):
            obv_config = {
                "tickers": watchlist,
                "sma_period": settings.OBV_SMA_PERIOD,
                "divergence_lookback": settings.OBV_DIVERGENCE_LOOKBACK,
                "min_price_change": settings.OBV_MIN_PRICE_CHANGE,
                "timeframe": tf,
                "confidence": 0.70
            }
            self.generators.append({
                "name": f"obv_{tf}",
                "generator": OBVSignalGenerator(obv_config),
                "interval": settings.OBV_CHECK_INTERVAL_SECONDS
            })
        
        # SAR generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.SAR_TIMEFRAME):
            sar_config = {
                "tickers": watchlist,
                "acceleration": settings.SAR_ACCELERATION,
                "maximum": settings.SAR_MAXIMUM,
                "timeframe": tf,
                "confidence": 0.75
            }
            self.generators.append({
                "name": f"sar_{tf}",
                "generator": SARSignalGenerator(sar_config),
                "interval": settings.SAR_CHECK_INTERVAL_SECONDS
            })
        
        # 200 EMA Crossover generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.EMA_200_TIMEFRAME):
            ema200_config = {
                "tickers": watchlist,
                "ema_period": settings.EMA_200_PERIOD,
                "timeframe": tf,
                "lookback_periods": 3,
                "confidence": 0.75
            }
            self.generators.append({
                "name": f"ema_200_crossover_{tf}",
                "generator": EMA200CrossoverSignalGenerator(ema200_config),
                "interval": settings.EMA_200_CHECK_INTERVAL_SECONDS
            })
        
        # Swing Point Break generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.SWING_POINT_TIMEFRAME):
            swing_config = {
                "tickers": watchlist,
                "lookback_periods": settings.SWING_POINT_LOOKBACK_PERIODS,
                "timeframe": tf,
                "min_swing_strength": settings.SWING_POINT_MIN_STRENGTH,
                "confidence": 0.80
            }
            self.generators.append({
                "name": f"swing_point_break_{tf}",
                "generator": SwingPointBreakSignalGenerator(swing_config),
                "interval": settings.SWING_POINT_CHECK_INTERVAL_SECONDS
            })
        
        # Momentum Divergence generator (RSI - multi-timeframe)
        for tf in self._merge_timeframes(settings.DIVERGENCE_TIMEFRAME):
            rsi_div_config = {
                "tickers": watchlist,
                "indicator": settings.DIVERGENCE_INDICATOR,
                "rsi_period": settings.DIVERGENCE_RSI_PERIOD,
                "lookback_periods": settings.DIVERGENCE_LOOKBACK_PERIODS,
                "timeframe": tf,
                "confidence": 0.85
            }
            self.generators.append({
                "name": f"rsi_divergence_{tf}",
                "generator": MomentumDivergenceSignalGenerator(rsi_div_config),
                "interval": settings.DIVERGENCE_CHECK_INTERVAL_SECONDS
            })
        
        # Fair Value Gap generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.FVG_TIMEFRAME):
            fvg_config = {
                "tickers": watchlist,
                "min_gap_pips": settings.FVG_MIN_GAP_PIPS,
                "timeframe": tf,
                "confidence": 0.80
            }
            self.generators.append({
                "name": f"fvg_{tf}",
                "generator": FairValueGapSignalGenerator(fvg_config),
                "interval": settings.FVG_CHECK_INTERVAL_SECONDS
            })
        
        # Liquidity Sweep generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.LIQUIDITY_SWEEP_TIMEFRAME):
            liq_sweep_config = {
                "tickers": watchlist,
                "lookback_periods": settings.LIQUIDITY_SWEEP_LOOKBACK_PERIODS,
                "sweep_tolerance_pips": settings.LIQUIDITY_SWEEP_TOLERANCE_PIPS,
                "timeframe": tf,
                "confidence": 0.85
            }
            self.generators.append({
                "name": f"liquidity_sweep_{tf}",
                "generator": LiquiditySweepSignalGenerator(liq_sweep_config),
                "interval": settings.LIQUIDITY_SWEEP_CHECK_INTERVAL_SECONDS
            })
        
        # Break of Structure generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.BOS_TIMEFRAME):
            bos_config = {
                "tickers": watchlist,
                "lookback_periods": settings.BOS_LOOKBACK_PERIODS,
                "min_swing_strength": settings.BOS_MIN_SWING_STRENGTH,
                "timeframe": tf,
                "confidence": 0.75
            }
            self.generators.append({
                "name": f"bos_{tf}",
                "generator": BreakOfStructureSignalGenerator(bos_config),
                "interval": settings.BOS_CHECK_INTERVAL_SECONDS
            })
        
        # Order Block generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.ORDER_BLOCK_TIMEFRAME):
            order_block_config = {
                "tickers": watchlist,
                "lookback_periods": settings.ORDER_BLOCK_LOOKBACK_PERIODS,
                "min_move_pips": settings.ORDER_BLOCK_MIN_MOVE_PIPS,
                "timeframe": tf,
                "confidence": 0.80
            }
            self.generators.append({
                "name": f"order_block_{tf}",
                "generator": OrderBlockSignalGenerator(order_block_config),
                "interval": settings.ORDER_BLOCK_CHECK_INTERVAL_SECONDS
            })
        
        # Change of Character generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.CHOCH_TIMEFRAME):
            choch_config = {
                "tickers": watchlist,
                "lookback_periods": settings.CHOCH_LOOKBACK_PERIODS,
                "min_swing_strength": settings.CHOCH_MIN_SWING_STRENGTH,
                "timeframe": tf,
                "confidence": 0.85
            }
            self.generators.append({
                "name": f"choch_{tf}",
                "generator": ChangeOfCharacterSignalGenerator(choch_config),
                "interval": settings.CHOCH_CHECK_INTERVAL_SECONDS
            })
        
        # Volume Profile POC Break generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.POC_TIMEFRAME):
            poc_config = {
                "tickers": watchlist,
                "lookback_periods": settings.POC_LOOKBACK_PERIODS,
                "timeframe": tf,
                "confidence": 0.75
            }
            self.generators.append({
                "name": f"poc_break_{tf}",
                "generator": VolumeProfilePOCSignalGenerator(poc_config),
                "interval": settings.POC_CHECK_INTERVAL_SECONDS
            })
        
        # Accumulation/Distribution generator (multi-timeframe)
        for tf in self._merge_timeframes(settings.ACCUM_DIST_TIMEFRAME):
            accum_dist_config = {
                "tickers": watchlist,
                "lookback_periods": settings.ACCUM_DIST_LOOKBACK_PERIODS,
                "min_slope_threshold": settings.ACCUM_DIST_MIN_SLOPE,
                "timeframe": tf,
                "confidence": 0.75
            }
            self.generators.append({
                "name": f"accum_dist_{tf}",
                "generator": AccumulationDistributionSignalGenerator(accum_dist_config),
                "interval": settings.ACCUM_DIST_CHECK_INTERVAL_SECONDS
            })
        
        # HTF Trend Alignment generator (multi-timeframe)
        htf_timeframes_list = [tf.strip() for tf in settings.HTF_TREND_TIMEFRAMES.split(",") if tf.strip()]
        for tf in self._merge_timeframes(settings.HTF_TREND_TIMEFRAME):
            htf_trend_config = {
                "tickers": watchlist,
                "ema_period": settings.HTF_TREND_EMA_PERIOD,
                "htf_timeframes": htf_timeframes_list,
                "min_alignment": settings.HTF_TREND_MIN_ALIGNMENT,
                "timeframe": tf,
                "confidence": 0.85
            }
            self.generators.append({
                "name": f"htf_trend_{tf}",
                "generator": HTFTrendAlignmentSignalGenerator(htf_trend_config),
                "interval": settings.HTF_TREND_CHECK_INTERVAL_SECONDS
            })
        
        logger.info(
            "generators_initialized",
            count=len(self.generators),
            generators=[g["name"] for g in self.generators]
        )
    
    async def run_generator(self, generator_info: dict):
        """
        Run a single generator in a loop.
        
        Automatically detects asset types from tickers and checks market hours.
        Skips generation only if ALL asset types in the watchlist are closed.
        
        Args:
            generator_info: Dict with generator, interval, and name
        """
        generator = generator_info["generator"]
        interval = generator_info["interval"]
        name = generator_info["name"]
        
        # Import market hours checker
        from app.utils.market_hours import MarketHoursChecker
        
        # Get tickers from generator config
        tickers = generator.config.get("tickers", [])
        
        logger.info(
            "generator_started",
            generator=name,
            interval_seconds=interval,
            ticker_count=len(tickers),
            market_hours_check_enabled=settings.ENABLE_MARKET_HOURS_CHECK
        )
        
        # Log initial market status for all asset types
        if settings.ENABLE_MARKET_HOURS_CHECK and tickers:
            # Detect asset types present
            from app.utils.market_hours import MarketType
            asset_types = set(MarketHoursChecker.detect_asset_type(t) for t in tickers)
            
            for asset_type in asset_types:
                status_msg = MarketHoursChecker.get_market_status_message(asset_type)
                logger.info("market_status_on_generator_start", generator=name, status=status_msg)
        
        while self.running:
            try:
                # âœ… NEW: Check if ANY ticker is tradeable (stocks, forex, or crypto)
                if settings.ENABLE_MARKET_HOURS_CHECK and tickers:
                    if not MarketHoursChecker.any_ticker_tradeable(tickers):
                        logger.debug(
                            "all_markets_closed_skipping_generation",
                            generator=name,
                            ticker_count=len(tickers)
                        )
                        # Use shorter interval when all markets are closed
                        await asyncio.sleep(settings.MARKET_HOURS_CHECK_INTERVAL_SECONDS)
                        continue
                
                # Track scan
                if self.meter:
                    self.generator_scans_total.add(1, {"generator": name})
                
                # Track generation time
                start_time = time.time()
                
                # Generate signals
                signals = await generator.generate()
                
                # Record generation duration (even if no signals)
                if self.meter:
                    gen_duration = time.time() - start_time
                    self.signal_generation_duration.record(
                        gen_duration,
                        {"generator": name}
                    )
                
                if signals:
                    # Emit signals
                    await self._emit_signals(signals, generator_name=name)
                
            except Exception as e:
                # Track error
                if self.meter:
                    self.generator_scan_errors.add(1, {
                        "generator": name,
                        "error_type": type(e).__name__
                    })
                
                logger.error(
                    "generator_error",
                    generator=name,
                    error=str(e),
                    exc_info=True
                )
            
            # Wait for next interval (use shorter interval if all markets closed)
            if settings.ENABLE_MARKET_HOURS_CHECK and tickers and not MarketHoursChecker.any_ticker_tradeable(tickers):
                await asyncio.sleep(settings.MARKET_HOURS_CHECK_INTERVAL_SECONDS)
            else:
                await asyncio.sleep(interval)
    
    async def _emit_signals(self, signals: List[Signal], generator_name: str = None):
        """
        Emit signals to output.
        
        Phase 1: Log to stdout/file
        Phase 2: Publish to Kafka
        
        For each signal ticker, enriches with pipeline_id and scanner_id if available.
        
        Args:
            signals: List of signals to emit
            generator_name: Name of the generator that produced signals
        """
        # Get pipeline-scanner mapping for enrichment
        pipeline_scanner_mapping = {}
        if self.scanner_universe:
            try:
                pipeline_scanner_mapping = self.scanner_universe.get_pipeline_scanner_mapping()
            except Exception as e:
                logger.warning("failed_to_get_pipeline_mapping", error=str(e))
        
        for signal in signals:
            # Enrich signal with pipeline routing info
            # Build a ticker->pipelines mapping
            ticker_to_pipelines = {}
            for ticker_signal in signal.tickers:
                ticker = ticker_signal.ticker
                
                # Find matching pipelines for this ticker
                matched_pipelines = []
                for pipeline_id, pipeline_data in pipeline_scanner_mapping.items():
                    if ticker in pipeline_data.get('tickers', []):
                        matched_pipelines.append({
                            'pipeline_id': pipeline_id,
                            'pipeline_name': pipeline_data['pipeline_name'],
                            'scanner_id': pipeline_data['scanner_id'],
                            'scanner_name': pipeline_data['scanner_name']
                        })
                
                if matched_pipelines:
                    ticker_to_pipelines[ticker] = matched_pipelines
            
            # Add pipeline routing info to signal metadata (not ticker metadata)
            if ticker_to_pipelines:
                if not signal.metadata:
                    signal.metadata = {}
                signal.metadata['ticker_pipelines'] = ticker_to_pipelines
            
            # Convert to Kafka-ready format
            message = signal.to_kafka_message()
            
            # Add to recent signals buffer for monitoring
            self._add_to_recent_signals(signal)
            
            # Track metrics
            if self.meter:
                self.signals_generated.add(1, {
                    "signal_type": signal.signal_type.value,
                    "source": signal.source,
                    "generator": generator_name or signal.source
                })
            
            # Phase 2: Publish to Kafka
            if self.kafka_producer:
                publish_start = time.time()
                try:
                    # Use signal_type as the key for partitioning
                    key = signal.signal_type.value
                    
                    future = self.kafka_producer.send(
                        settings.KAFKA_SIGNAL_TOPIC,
                        key=key,
                        value=message
                    )
                    
                    # Wait for acknowledgment (with timeout)
                    record_metadata = future.get(timeout=10)
                    
                    # Record successful publish
                    if self.meter:
                        publish_duration = time.time() - publish_start
                        self.kafka_publish_duration.record(publish_duration, {
                            "signal_type": signal.signal_type.value
                        })
                        self.kafka_publish_success.add(1, {
                            "signal_type": signal.signal_type.value
                        })
                    
                    logger.info(
                        "signal_published_to_kafka",
                        signal_id=str(signal.signal_id),
                        topic=record_metadata.topic,
                        partition=record_metadata.partition,
                        offset=record_metadata.offset
                    )
                except KafkaError as e:
                    # Record failed publish
                    if self.meter:
                        self.kafka_publish_failure.add(1, {
                            "signal_type": signal.signal_type.value,
                            "error_type": type(e).__name__
                        })
                    
                    logger.error(
                        "kafka_publish_failed",
                        signal_id=str(signal.signal_id),
                        error=str(e),
                        exc_info=True
                    )
                except Exception as e:
                    # Record failed publish
                    if self.meter:
                        self.kafka_publish_failure.add(1, {
                            "signal_type": signal.signal_type.value,
                            "error_type": type(e).__name__
                        })
                    
                    logger.error(
                        "unexpected_kafka_error",
                        signal_id=str(signal.signal_id),
                        error=str(e),
                        exc_info=True
                    )
            
            # Also log the signal (structured logging)
            logger.info(
                "signal_emitted",
                signal_id=str(signal.signal_id),
                signal_type=signal.signal_type.value,
                source=signal.source,
                timestamp=int(signal.timestamp.timestamp()),
                tickers=[
                    {
                        "ticker": ts.ticker,
                        "signal": ts.signal.value,
                        "confidence": ts.confidence
                    }
                    for ts in signal.tickers
                ]
            )
            
            # Also print to stdout in a nice format for visibility
            print("\n" + "="*80)
            print(f"ðŸ”” SIGNAL GENERATED: {signal.signal_type.value.upper()}")
            print(f"   ID: {signal.signal_id}")
            print(f"   Source: {signal.source}")
            print(f"   Timestamp: {signal.timestamp.strftime('%Y-%m-%d %H:%M:%S UTC')}")
            
            if self.kafka_producer:
                print(f"   ðŸ“¤ Published to Kafka: {settings.KAFKA_SIGNAL_TOPIC}")
            
            print(f"   Tickers:")
            
            for ticker_signal in signal.tickers:
                print(f"     â€¢ {ticker_signal.ticker}: {ticker_signal.signal.value} (confidence: {ticker_signal.confidence:.0f}%)")
                if ticker_signal.reasoning:
                    print(f"       â†’ {ticker_signal.reasoning}")
            
            if signal.metadata:
                print(f"   Metadata: {json.dumps(signal.metadata, indent=6)}")
            
            print("="*80 + "\n")
    
    async def start(self):
        """Start all generators."""
        self.running = True
        
        logger.info(
            "signal_generator_service_starting",
            service=settings.SERVICE_NAME,
            generators=len(self.generators)
        )
        
        print(f"\nðŸš€ Signal Generator Service Starting...")
        print(f"   Generators: {len(self.generators)}")
        print(f"   Watchlist: {settings.get_watchlist()}")
        print(f"   Log Level: {settings.LOG_LEVEL}\n")
        
        # Start all generators as concurrent tasks
        tasks = [
            asyncio.create_task(self.run_generator(gen_info))
            for gen_info in self.generators
        ]
        
        # Add universe refresh task if Scanner Universe Manager is enabled
        if self.scanner_universe:
            tasks.append(asyncio.create_task(self._universe_refresh_loop()))
            logger.info("universe_refresh_task_started", 
                       interval_seconds=settings.UNIVERSE_REFRESH_INTERVAL_SECONDS)
        
        # Wait for all tasks (runs indefinitely until interrupted)
        try:
            await asyncio.gather(*tasks)
        except asyncio.CancelledError:
            logger.info("signal_generator_service_cancelled")
            raise
    
    async def _universe_refresh_loop(self):
        """Periodically refresh the ticker universe from active scanners."""
        while self.running:
            try:
                await asyncio.sleep(settings.UNIVERSE_REFRESH_INTERVAL_SECONDS)
                
                if not self.scanner_universe:
                    break
                
                # Refresh universe
                new_tickers = self.scanner_universe.get_active_scanner_tickers()
                old_tickers = set(settings.get_watchlist())
                
                if set(new_tickers) != old_tickers:
                    logger.info(
                        "universe_changed_reinitializing_generators",
                        old_count=len(old_tickers),
                        new_count=len(new_tickers),
                        added=list(set(new_tickers) - old_tickers),
                        removed=list(old_tickers - set(new_tickers))
                    )
                    
                    # Re-initialize generators with new watchlist
                    self._initialize_generators()
                else:
                    logger.debug("universe_unchanged", ticker_count=len(new_tickers))
                
            except Exception as e:
                logger.error("universe_refresh_error", error=str(e), exc_info=True)
                # Continue running even if refresh fails
                await asyncio.sleep(60)  # Wait 1 minute before retrying
    
    def _add_to_recent_signals(self, signal: Signal):
        """Add signal to recent signals buffer for monitoring."""
        try:
            signal_info = {
                "signal_id": str(signal.signal_id),
                "signal_type": signal.signal_type.value,
                "timestamp": signal.timestamp.isoformat() if signal.timestamp else datetime.utcnow().isoformat(),
                "source": signal.source,
                "tickers": [
                    {
                        "ticker": ts.ticker,
                        "signal": ts.signal.value,
                        "confidence": ts.confidence,
                        "reasoning": ts.reasoning
                    }
                    for ts in signal.tickers
                ],
                "metadata": signal.metadata or {}
            }
            self.recent_signals.append(signal_info)
        except Exception as e:
            logger.error("failed_to_add_signal_to_buffer", error=str(e))
    
    def get_recent_signals(self, limit: int = 50) -> List[dict]:
        """
        Get recent signals for monitoring.
        
        Args:
            limit: Maximum number of signals to return
            
        Returns:
            List of recent signals
        """
        return list(self.recent_signals)[-limit:]
    
    async def stop(self):
        """Stop all generators and cleanup resources."""
        self.running = False
        
        # Close Kafka producer
        if self.kafka_producer:
            try:
                self.kafka_producer.flush(timeout=5)
                self.kafka_producer.close(timeout=5)
                logger.info("kafka_producer_closed")
            except Exception as e:
                logger.error("kafka_producer_close_error", error=str(e))
        
        logger.info("signal_generator_service_stopping")
        print("\nðŸ›‘ Signal Generator Service Stopping...\n")


async def main():
    """Main entry point."""
    service = SignalGeneratorService()
    
    # Set the global service instance for API access
    from app.api import set_service_instance
    set_service_instance(service)
    
    # Start HTTP API server
    import uvicorn
    from app.api import app
    
    # Start API server in background
    api_server = uvicorn.Server(
        uvicorn.Config(
            app,
            host="0.0.0.0",
            port=8007,
            log_level="info"
        )
    )
    
    async def start_api():
        """Start the API server."""
        await api_server.serve()
    
    # Start both API server and signal generation
    try:
        await asyncio.gather(
            start_api(),
            service.start()
        )
    except KeyboardInterrupt:
        logger.info("keyboard_interrupt_received")
        await service.stop()
    except Exception as e:
        logger.error("service_crashed", error=str(e), exc_info=True)
        await service.stop()
        raise


if __name__ == "__main__":
    asyncio.run(main())

